{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5769141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def accuracy(predictions, true_labels):\n",
    "    return (predictions[predictions == true_labels].size * 100) / predictions.size\n",
    "\n",
    "\n",
    "def test_accuracy(predictions, true_labels):\n",
    "    print(\"test_accuracy: \", accuracy(predictions, true_labels))\n",
    "\n",
    "\n",
    "def train_accuracy(predictions, true_labels):\n",
    "    print(\"train_accuracy: \", accuracy(predictions, true_labels))\n",
    "\n",
    "\n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "\n",
    "    def __init__(self, depth=None):\n",
    "        self.depth = depth\n",
    "        self.root = self.Node(True, False)\n",
    "        self.features = None\n",
    "        self.label = None\n",
    "        self.features_uniq_vals = defaultdict(list)\n",
    "        self.label_uniq_vals = None\n",
    "        self.most_frequent_label = None\n",
    "\n",
    "    def set_depth(self, depth):\n",
    "        self.depth = depth\n",
    "\n",
    "    class Node:\n",
    "\n",
    "        def __init__(self, is_root=False, is_leaf=True):\n",
    "            self.is_root = is_root\n",
    "            self.is_leaf = is_leaf\n",
    "            # feature_value:child_node\n",
    "            self.child = {}\n",
    "            self.feature = None\n",
    "            self.label = None\n",
    "            self.parent_feat_info = None\n",
    "            self.entropy = None\n",
    "            self.info_gain = None\n",
    "\n",
    "    def preprocess_data(self, x, y):\n",
    "\n",
    "        self.features = x.columns\n",
    "        self.label = y.name\n",
    "\n",
    "        for feat in self.features:\n",
    "            self.features_uniq_vals[feat] = list(x[feat].unique())\n",
    "        # print(self.features_uniq_vals)\n",
    "        self.label_uniq_vals = list(y.unique())\n",
    "        self.most_frequent_label = y.mode().loc[0]\n",
    "\n",
    "    # @staticmethod\n",
    "    def entropy(self, y):\n",
    "        return sum((-y.value_counts() / y.size) * np.log2(y.value_counts() / y.size))\n",
    "\n",
    "    def information_gain(self, x, y, feature):\n",
    "        h_x = self.entropy(y)\n",
    "        expected_new_h_x = 0\n",
    "\n",
    "        for v in x[feature].unique():\n",
    "            expected_new_h_x += ((y[x[feature] == v].size/y.size) *\n",
    "                                 self.entropy(y[x[feature] == v]))\n",
    "\n",
    "        return h_x - expected_new_h_x\n",
    "\n",
    "    def get_most_informative_feature(self, x, y, ancestors):\n",
    "\n",
    "        highest_info_gain = 0\n",
    "        most_informative_feat = None\n",
    "        for feature in x.columns:\n",
    "            if feature not in ancestors:\n",
    "                feature_info_gain = self.information_gain(x, y, feature)\n",
    "                if feature_info_gain > highest_info_gain:\n",
    "                    highest_info_gain = feature_info_gain\n",
    "                    most_informative_feat = feature\n",
    "\n",
    "        return most_informative_feat\n",
    "\n",
    "    def build_decision_tree(self, x, y, node, depth, ancestors):\n",
    "        # if depth of tree has reached limit\n",
    "        # if all labels in the current x are same, then no need to build the tree further\n",
    "        # if all features are already used, then the current node has to be a leaf\n",
    "        # if all features have only one unique value and len(anc) != #features ==> info_gain = 0\n",
    "        if depth == self.depth or \\\n",
    "                self.entropy(y) == 0 or \\\n",
    "                len(ancestors) == len(x.columns) or \\\n",
    "                not self.get_most_informative_feature(x, y, ancestors):\n",
    "            node.is_leaf = True\n",
    "            node.label = y.mode().loc[0]\n",
    "            return\n",
    "\n",
    "        node.is_leaf = False\n",
    "        node.feature = self.get_most_informative_feature(x, y, ancestors)\n",
    "        node.info_gain = self.information_gain(x, y, node.feature)\n",
    "        node.entropy = self.entropy(y)\n",
    "\n",
    "        ancestors = ancestors + [node.feature]\n",
    "\n",
    "        for val in x[node.feature].unique():\n",
    "            node.child[val] = self.Node()\n",
    "            # setting parent feature value\n",
    "            node.child[val].parent_feat_info = (node.feature, val)\n",
    "            self.build_decision_tree(x[x[node.feature] == val],\n",
    "                                     y[x[node.feature] == val],\n",
    "                                     node.child[val],\n",
    "                                     depth+1, ancestors)\n",
    "\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self.preprocess_data(x, y)\n",
    "        self.build_decision_tree(x, y, self.root, 0, [])\n",
    "        return\n",
    "\n",
    "    def predict(self, x):\n",
    "        predictions = []\n",
    "        for idx, sample in x.iterrows():\n",
    "            node = self.root\n",
    "            predicted = False\n",
    "            while not node.is_leaf:\n",
    "                if sample[node.feature] in node.child:\n",
    "                    node = node.child[sample[node.feature]]\n",
    "                else:\n",
    "                    predictions.append('NaN')\n",
    "                    predicted = True\n",
    "                    break\n",
    "            if not predicted:\n",
    "                predictions.append(node.label)\n",
    "\n",
    "        return pd.Series(predictions)\n",
    "\n",
    "    def plot_tree(self, plt):\n",
    "\n",
    "        if self.root.entropy:\n",
    "            print(\"Entropy of data:\", self.root.entropy)\n",
    "\n",
    "        print(\"Best feature is:\", self.root.feature,\n",
    "              \"and it's information gain is:\", self.root.info_gain)\n",
    "\n",
    "        if plt:\n",
    "            print(\"$$$$$$$$$PLOTTING TREE$$$$$$$$$$$$$$$$$$$\")\n",
    "            queue = [self.root]\n",
    "            num_nodes_in_curr_level = 0\n",
    "            num_nodes_in_next_level = 1\n",
    "            depth = -1\n",
    "            while len(queue) != 0:\n",
    "\n",
    "                if num_nodes_in_curr_level == 0:\n",
    "                    depth += 1\n",
    "                    print(\"At Depth: \", depth)\n",
    "                    num_nodes_in_curr_level = num_nodes_in_next_level\n",
    "                    num_nodes_in_next_level = 0\n",
    "\n",
    "                node = queue.pop(0)\n",
    "                num_nodes_in_curr_level -= 1\n",
    "\n",
    "                if node.is_root:\n",
    "                    print(\"**Root**\")\n",
    "                if not node.is_root:\n",
    "                    print(\"parent_feat: {}, parent_feat_val:{}\"\n",
    "                          .format(node.parent_feat_info[0],\n",
    "                                  node.parent_feat_info[1]))\n",
    "                if not node.is_leaf:\n",
    "                    # print(\"**Root**, Feature: {}\".format(node.feature))\n",
    "                    print(\"Feature: {}, Entropy: {}, info_gain: {}\"\n",
    "                          .format(node.feature, node.entropy, node.info_gain))\n",
    "                if node.is_leaf:\n",
    "                    print(\"**Leaf** Label:\", node.label)\n",
    "\n",
    "                for feat_val, child in node.child.items():\n",
    "                    queue.append(child)\n",
    "                    num_nodes_in_next_level += 1\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_data = pd.read_csv('data/roberta.train.csv'')\n",
    "test_data = pd.read_csv('data/roberta.test.csv'')\n",
    "\n",
    "x_train = train_data[train_data.columns[:-1]]\n",
    "y_train = train_data[train_data.columns[-1]]\n",
    "\n",
    "x_test = test_data[x_train.columns]\n",
    "y_test = test_data[y_train.name]\n",
    "\n",
    "# Baseline model\n",
    "print(\"********Baseline model************************\")\n",
    "clf = DecisionTreeClassifier(depth=0)\n",
    "clf.fit(x_train, y_train)\n",
    "clf.plot_tree(False)\n",
    "\n",
    "predictions_train = clf.predict(x_train)\n",
    "predictions_test = clf.predict(x_test)\n",
    "\n",
    "train_accuracy(predictions_train, y_train)\n",
    "test_accuracy(predictions_test, y_test)\n",
    "\n",
    "# Full Trees\n",
    "# Since, there are only 6 features, I'm passing depth as 8, so that it is always\n",
    "# greater than the max depth possible\n",
    "print(\"********Full Tree************************\")\n",
    "clf = DecisionTreeClassifier(depth=8)\n",
    "clf.fit(x_train, y_train)\n",
    "clf.plot_tree(False)\n",
    "\n",
    "predictions_train = clf.predict(x_train)\n",
    "predictions_test = clf.predict(x_test)\n",
    "\n",
    "train_accuracy(predictions_train, y_train)\n",
    "test_accuracy(predictions_test, y_test)\n",
    "\n",
    "# Limiting Depth\n",
    "print(\"********Limiting Depth************************\")\n",
    "clf = DecisionTreeClassifier()\n",
    "k_fold_cv = KFoldCrossValidation()\n",
    "\n",
    "depths = [d for d in range(1, 6)]\n",
    "best_depth = k_fold_cv.cross_val_score(clf, depths)\n",
    "\n",
    "print(\"Best Depth:\", best_depth)\n",
    "\n",
    "clf.set_depth(best_depth)\n",
    "clf.fit(x_train, y_train)\n",
    "clf.plot_tree(False)\n",
    "\n",
    "predictions_train = clf.predict(x_train)\n",
    "predictions_test = clf.predict(x_test)\n",
    "\n",
    "train_accuracy(predictions_train, y_train)\n",
    "test_accuracy(predictions_test, y_test)\n",
    "\n",
    "# Missing Data\n",
    "print(\"********Replacing missing data**************\")\n",
    "\n",
    "# part a\n",
    "print(\"a: Replacing \\\"?\\\" with the most common feature value in the whole dataset\")\n",
    "train_data = pd.read_csv('data/data_missing/train.csv')\n",
    "test_data = pd.read_csv('data/data_missing/test.csv')\n",
    "\n",
    "replace1(train_data)\n",
    "replace1(test_data)\n",
    "\n",
    "x_train = train_data[train_data.columns[:-1]]\n",
    "y_train = train_data['label']\n",
    "\n",
    "x_test = test_data[x_train.columns]\n",
    "y_test = test_data[y_train.name]\n",
    "\n",
    "clf = DecisionTreeClassifier(depth=8)\n",
    "clf.fit(x_train, y_train)\n",
    "clf.plot_tree(False)\n",
    "\n",
    "predictions_train = clf.predict(x_train)\n",
    "predictions_test = clf.predict(x_test)\n",
    "\n",
    "train_accuracy(predictions_train, y_train)\n",
    "test_accuracy(predictions_test, y_test)\n",
    "\n",
    "# part b\n",
    "print(\"b: Replacing \\\"?\\\" with the most common feature value for the same label\")\n",
    "train_data = pd.read_csv('data/data_missing/train.csv')\n",
    "test_data = pd.read_csv('data/data_missing/test.csv')\n",
    "\n",
    "replace2(train_data)\n",
    "replace2(test_data)\n",
    "\n",
    "x_train = train_data[train_data.columns[:-1]]\n",
    "y_train = train_data['label']\n",
    "\n",
    "x_test = test_data[x_train.columns]\n",
    "y_test = test_data[y_train.name]\n",
    "\n",
    "clf = DecisionTreeClassifier(depth=8)\n",
    "clf.fit(x_train, y_train)\n",
    "clf.plot_tree(False)\n",
    "\n",
    "predictions_train = clf.predict(x_train)\n",
    "predictions_test = clf.predict(x_test)\n",
    "\n",
    "train_accuracy(predictions_train, y_train)\n",
    "test_accuracy(predictions_test, y_test)\n",
    "\n",
    "# plotting the Full Tree\n",
    "print(\"Plotting the full tree implementation:\")\n",
    "\n",
    "train_data = pd.read_csv('data/data/train.csv')\n",
    "test_data = pd.read_csv('data/data/test.csv')\n",
    "\n",
    "x_train = train_data[train_data.columns[:-1]]\n",
    "y_train = train_data[train_data.columns[-1]]\n",
    "\n",
    "x_test = test_data[x_train.columns]\n",
    "y_test = test_data[y_train.name]\n",
    "\n",
    "clf = DecisionTreeClassifier(depth=8)\n",
    "clf.fit(x_train, y_train)\n",
    "clf.plot_tree(True)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
